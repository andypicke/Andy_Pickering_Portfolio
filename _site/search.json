[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow has time-of-use pricing affected our home electricity use?\n\n\n\n\n\n\nEV\n\n\nR\n\n\nvisualization\n\n\nenergy\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the U.S. Large-Scale Solar Photovoltaic Database (USPVDB) in R\n\n\n\n\n\n\nvisualization\n\n\nmapping\n\n\nenergy\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Trends in Heating and Cooling Degree days using R\n\n\n\n\n\n\nenergy\n\n\nR\n\n\nweather\n\n\nclimate\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nTrends in US Electricity Generation and CO2 Emissions\n\n\n\n\n\n\nenergy\n\n\nR\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the Number of EV Charging Stations by County in Colorado Using R\n\n\n\n\n\n\nEV\n\n\nR\n\n\nvisualization\n\n\nmapping\n\n\nAPI\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating State Electricity Generation By Fuel Type using R\n\n\n\n\n\n\nenergy\n\n\nEIA\n\n\nR\n\n\nvisualization\n\n\nAPI\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Severe Thunderstorm Outlook in R using Leaflet\n\n\n\n\n\n\nweather\n\n\nR\n\n\nvisualization\n\n\nmapping\n\n\ngeospatial\n\n\nleaflet\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEV Charging Stations Analysis\n\n\n\n\n\n\nenergy\n\n\nEV\n\n\nR\n\n\nvisualization\n\n\nAPI\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday Energy Analysis\n\n\n\n\n\n\nenergy\n\n\nTidyTuesday\n\n\nR\n\n\nvisualization\n\n\n\n\n\n\n\n\n\nJun 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nAndy Pickering\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome! I am a data scientist interested in energy and the environment (among many other topics). On this website you can find my portfolio and resume, as well as blog posts about interesting analyses I am working on. I’ve also created a list of resources that I’ve found helpful in learning data science or finding data sets. Enjoy!"
  },
  {
    "objectID": "posts/Energy_Tidy_Tuesday/index.html",
    "href": "posts/Energy_Tidy_Tuesday/index.html",
    "title": "Tidy Tuesday Energy Analysis",
    "section": "",
    "text": "In this document I’ll analyze and visualize some energy data that were the focus of Tidy Tuesday 2023 week 23. The data comes from Our World In Data and the full data set is available here. Data Source Citation: Ritchie, Roser, and Rosado (2022).\n\nI’ll start by loading the tidyverse (Wickham et al. (2019)) library and the data set. The result is a dataframe with a row for each country and year, from 1900-2002.\n\nCodesuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(plotly))\nggplot2::theme_set(theme_grey(base_size = 15))\n\nowid_energy &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-06/owid-energy.csv\", show_col_types = FALSE)\n\nhead(owid_energy)\n\n# A tibble: 6 × 129\n  country      year iso_code population   gdp biofuel_cons_change_pct\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;\n1 Afghanistan  1900 AFG         4832414    NA                      NA\n2 Afghanistan  1901 AFG         4879685    NA                      NA\n3 Afghanistan  1902 AFG         4935122    NA                      NA\n4 Afghanistan  1903 AFG         4998861    NA                      NA\n5 Afghanistan  1904 AFG         5063419    NA                      NA\n6 Afghanistan  1905 AFG         5128808    NA                      NA\n# ℹ 123 more variables: biofuel_cons_change_twh &lt;dbl&gt;,\n#   biofuel_cons_per_capita &lt;dbl&gt;, biofuel_consumption &lt;dbl&gt;,\n#   biofuel_elec_per_capita &lt;dbl&gt;, biofuel_electricity &lt;dbl&gt;,\n#   biofuel_share_elec &lt;dbl&gt;, biofuel_share_energy &lt;dbl&gt;,\n#   carbon_intensity_elec &lt;dbl&gt;, coal_cons_change_pct &lt;dbl&gt;,\n#   coal_cons_change_twh &lt;dbl&gt;, coal_cons_per_capita &lt;dbl&gt;,\n#   coal_consumption &lt;dbl&gt;, coal_elec_per_capita &lt;dbl&gt;, …\n\n\n\nHow many countries are in the dataset?\n\n\nCodelength(unique(owid_energy$country))\n\n[1] 306\n\n\nThat’s a lot! I’ll focus on just the United States for now.\n\nI also noticed that the data set goes back to 1900 but a a lot of the data for earlier years are missing/NA so I’ll filter those out as well.\nIt looks like we have data for the USA from 2000-2021.\n\nMake a new dataframe for just the USA data and remove years without data:\n\nCodeusa &lt;- owid_energy %&gt;%\n  filter(country == \"United States\") %&gt;%\n  filter(!is.na(electricity_demand))\n\nusa |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\nFor this analysis, I’ve chosen to investigate how the mix of fuel types used to generate electricity has changed over time. We need to reduce carbon emissions in order to prevent or mitigate the effects of climate change, and electricity generation is a large component of these emissions. I’m interested to see what progress has been made in transitioning to more renewable/low-carbon fuels for electricity generation.\nConveniently, the data already contain fields for the share of total electricity generation for each fuel type! I’ll make a new data frame with just these fields. I can select these columns (all ending in share_elec), using the ends_with function from the dplyr (Wickham et al. (2023)) package.\n\nCodeusa %&gt;%\n  select(year, dplyr::ends_with(\"share_elec\")) %&gt;%\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 1: Percent of electricity generation by fueltype\n\n\n\nNow I have a dataframe (Table 1) with just the variables I want to plot, but the format isn’t ideal; I would have to specify/plot each variable separately. What I want to do is give a single plot command and group/color the lines by the variable (fuel type). To achieve this, I am going to pivot the data frame from wide to long format, using the pivot_longer function from the tidyr (Wickham, Vaughan, and Girlich (2023)) package.\n\n\n\n\n\n\nNote\n\n\n\nThe pivot_ functions in tidyr (Wickham, Vaughan, and Girlich (2023)) package were previously referred to as gather or melt.\n\n\n\nCodeusa_share &lt;- usa %&gt;%\n  select(year, ends_with(\"share_elec\")) %&gt;%\n  tidyr::pivot_longer(\n    cols = ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\") |&gt;\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) \n\nusa_share |&gt;\n    DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 2: Percent of electricity generation by fueltype\n\n\n\nNow my dataframe (Table 2) has a row for each year, fuel type, and value, and I can simply group or color by the fuel type when I plot.\n\nCodeg &lt;- usa_share %&gt;%\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation By Fuel type\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 1: Timeseries of the percent of total US electricty generation by fuel type.\n\n\n\nHere we finally have a plot (Figure 1) of the share of electricity generation by fuel type. We can see that the share of fossil fuels and coal has decreased, and renewable have increased. But there’s a lot on this plot and it’s hard to read, so I’ll focus on some more specific subsets of the data.\n\nFirst we can look at the total shares of fossil (oil,coal, gas), renewable (wind, solar, hydro), and nuclear generation. Grouping into these categories de-clutters the plot and makes it easier to interpret.\n\nCodeg &lt;- usa %&gt;%\n  select(year, fossil_share_elec, renewables_share_elec, nuclear_share_elec) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 2: Timeseries of the percent of total US electricty generation by fuel types\n\n\n\nObservations from this plot (Figure 2):\n\nFossil fuel share has been decreasing steadily since about 2007\nRenewable share has been increasing steadily since about 2007\nNuclear has remained relatively constant at around 20%.\nFossil share remains the majority of generation, but is decreasing. Renewables became approximately equal to nuclear around 2020 and are continuing to increase.\n\n\nIn this dataset, fossil fuels include coal, gas, and oil.\n\n\nCodeg &lt;- usa %&gt;%\n  select(year, fossil_share_elec, oil_share_elec, gas_share_elec, coal_share_elec) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation: Fossil Fuels\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 3: Timeseries of the percent of total US electricty generation by fossil fuels\n\n\n\nObservations from this plot (Figure 3):\n\nWe can see that the fossil fuel share of electricity generation has been decreasing, starting around 2008.\nCoal and gas make up the majority of the fossil fuel generation.\nCoal share has been decreasing while the gas share has increased. Coal was much higher than gas previously, but their shares became equal around 2015 and gas now makes up a larger share of the fossil fuel generation.\n\n\nIn this dataset, renewables include wind, solar, and hydro.\n\n\nCodeg &lt;- usa %&gt;%\n  select(\n    year, renewables_share_elec, solar_share_elec,\n    hydro_share_elec, wind_share_elec\n  ) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation: Renewables\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 4: Timeseries of the percent of total US electricty generation renewable fuels\n\n\n\nObservations from this plot (Figure 4):\n\nThe share of renewable electricity production has increased sharply, approximately doubling from 2008 to 2020.\nThe share of hydro generation has remained relatively constant.\n\nSolar and wind shares have increased significantly.\n\nWind started to increase earlier, around 2005.\nSolar started increasing around 2012"
  },
  {
    "objectID": "posts/Energy_Tidy_Tuesday/index.html#introduction",
    "href": "posts/Energy_Tidy_Tuesday/index.html#introduction",
    "title": "Tidy Tuesday Energy Analysis",
    "section": "",
    "text": "In this document I’ll analyze and visualize some energy data that were the focus of Tidy Tuesday 2023 week 23. The data comes from Our World In Data and the full data set is available here. Data Source Citation: Ritchie, Roser, and Rosado (2022)."
  },
  {
    "objectID": "posts/Energy_Tidy_Tuesday/index.html#analysis-and-visualization",
    "href": "posts/Energy_Tidy_Tuesday/index.html#analysis-and-visualization",
    "title": "Tidy Tuesday Energy Analysis",
    "section": "",
    "text": "I’ll start by loading the tidyverse (Wickham et al. (2019)) library and the data set. The result is a dataframe with a row for each country and year, from 1900-2002.\n\nCodesuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(plotly))\nggplot2::theme_set(theme_grey(base_size = 15))\n\nowid_energy &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-06-06/owid-energy.csv\", show_col_types = FALSE)\n\nhead(owid_energy)\n\n# A tibble: 6 × 129\n  country      year iso_code population   gdp biofuel_cons_change_pct\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;                   &lt;dbl&gt;\n1 Afghanistan  1900 AFG         4832414    NA                      NA\n2 Afghanistan  1901 AFG         4879685    NA                      NA\n3 Afghanistan  1902 AFG         4935122    NA                      NA\n4 Afghanistan  1903 AFG         4998861    NA                      NA\n5 Afghanistan  1904 AFG         5063419    NA                      NA\n6 Afghanistan  1905 AFG         5128808    NA                      NA\n# ℹ 123 more variables: biofuel_cons_change_twh &lt;dbl&gt;,\n#   biofuel_cons_per_capita &lt;dbl&gt;, biofuel_consumption &lt;dbl&gt;,\n#   biofuel_elec_per_capita &lt;dbl&gt;, biofuel_electricity &lt;dbl&gt;,\n#   biofuel_share_elec &lt;dbl&gt;, biofuel_share_energy &lt;dbl&gt;,\n#   carbon_intensity_elec &lt;dbl&gt;, coal_cons_change_pct &lt;dbl&gt;,\n#   coal_cons_change_twh &lt;dbl&gt;, coal_cons_per_capita &lt;dbl&gt;,\n#   coal_consumption &lt;dbl&gt;, coal_elec_per_capita &lt;dbl&gt;, …\n\n\n\nHow many countries are in the dataset?\n\n\nCodelength(unique(owid_energy$country))\n\n[1] 306\n\n\nThat’s a lot! I’ll focus on just the United States for now.\n\nI also noticed that the data set goes back to 1900 but a a lot of the data for earlier years are missing/NA so I’ll filter those out as well.\nIt looks like we have data for the USA from 2000-2021.\n\nMake a new dataframe for just the USA data and remove years without data:\n\nCodeusa &lt;- owid_energy %&gt;%\n  filter(country == \"United States\") %&gt;%\n  filter(!is.na(electricity_demand))\n\nusa |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\nFor this analysis, I’ve chosen to investigate how the mix of fuel types used to generate electricity has changed over time. We need to reduce carbon emissions in order to prevent or mitigate the effects of climate change, and electricity generation is a large component of these emissions. I’m interested to see what progress has been made in transitioning to more renewable/low-carbon fuels for electricity generation.\nConveniently, the data already contain fields for the share of total electricity generation for each fuel type! I’ll make a new data frame with just these fields. I can select these columns (all ending in share_elec), using the ends_with function from the dplyr (Wickham et al. (2023)) package.\n\nCodeusa %&gt;%\n  select(year, dplyr::ends_with(\"share_elec\")) %&gt;%\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 1: Percent of electricity generation by fueltype\n\n\n\nNow I have a dataframe (Table 1) with just the variables I want to plot, but the format isn’t ideal; I would have to specify/plot each variable separately. What I want to do is give a single plot command and group/color the lines by the variable (fuel type). To achieve this, I am going to pivot the data frame from wide to long format, using the pivot_longer function from the tidyr (Wickham, Vaughan, and Girlich (2023)) package.\n\n\n\n\n\n\nNote\n\n\n\nThe pivot_ functions in tidyr (Wickham, Vaughan, and Girlich (2023)) package were previously referred to as gather or melt.\n\n\n\nCodeusa_share &lt;- usa %&gt;%\n  select(year, ends_with(\"share_elec\")) %&gt;%\n  tidyr::pivot_longer(\n    cols = ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\") |&gt;\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) \n\nusa_share |&gt;\n    DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 2: Percent of electricity generation by fueltype\n\n\n\nNow my dataframe (Table 2) has a row for each year, fuel type, and value, and I can simply group or color by the fuel type when I plot.\n\nCodeg &lt;- usa_share %&gt;%\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation By Fuel type\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 1: Timeseries of the percent of total US electricty generation by fuel type.\n\n\n\nHere we finally have a plot (Figure 1) of the share of electricity generation by fuel type. We can see that the share of fossil fuels and coal has decreased, and renewable have increased. But there’s a lot on this plot and it’s hard to read, so I’ll focus on some more specific subsets of the data.\n\nFirst we can look at the total shares of fossil (oil,coal, gas), renewable (wind, solar, hydro), and nuclear generation. Grouping into these categories de-clutters the plot and makes it easier to interpret.\n\nCodeg &lt;- usa %&gt;%\n  select(year, fossil_share_elec, renewables_share_elec, nuclear_share_elec) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 2: Timeseries of the percent of total US electricty generation by fuel types\n\n\n\nObservations from this plot (Figure 2):\n\nFossil fuel share has been decreasing steadily since about 2007\nRenewable share has been increasing steadily since about 2007\nNuclear has remained relatively constant at around 20%.\nFossil share remains the majority of generation, but is decreasing. Renewables became approximately equal to nuclear around 2020 and are continuing to increase.\n\n\nIn this dataset, fossil fuels include coal, gas, and oil.\n\n\nCodeg &lt;- usa %&gt;%\n  select(year, fossil_share_elec, oil_share_elec, gas_share_elec, coal_share_elec) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation: Fossil Fuels\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 3: Timeseries of the percent of total US electricty generation by fossil fuels\n\n\n\nObservations from this plot (Figure 3):\n\nWe can see that the fossil fuel share of electricity generation has been decreasing, starting around 2008.\nCoal and gas make up the majority of the fossil fuel generation.\nCoal share has been decreasing while the gas share has increased. Coal was much higher than gas previously, but their shares became equal around 2015 and gas now makes up a larger share of the fossil fuel generation.\n\n\nIn this dataset, renewables include wind, solar, and hydro.\n\n\nCodeg &lt;- usa %&gt;%\n  select(\n    year, renewables_share_elec, solar_share_elec,\n    hydro_share_elec, wind_share_elec\n  ) %&gt;%\n  tidyr::pivot_longer(\n    cols = dplyr::ends_with(\"share_elec\"),\n    names_to = \"FuelType\",\n    values_to = \"Percentage\"\n  ) %&gt;%\n  mutate(FuelType = str_remove(FuelType,'_share_elec')) |&gt;\n  ggplot(aes(year, Percentage)) +\n  geom_line(aes(color = FuelType), linewidth = 1.5) +\n  ggtitle(\"Percent of US electricity Generation: Renewables\") +\n  xlab(\"Year\") +\n  ylab(\"Percent\")\n\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 4: Timeseries of the percent of total US electricty generation renewable fuels\n\n\n\nObservations from this plot (Figure 4):\n\nThe share of renewable electricity production has increased sharply, approximately doubling from 2008 to 2020.\nThe share of hydro generation has remained relatively constant.\n\nSolar and wind shares have increased significantly.\n\nWind started to increase earlier, around 2005.\nSolar started increasing around 2012"
  },
  {
    "objectID": "posts/Storm_Prediction_Center/Severe_Weather_Mapping.html",
    "href": "posts/Storm_Prediction_Center/Severe_Weather_Mapping.html",
    "title": "Mapping Severe Thunderstorm Outlook in R using Leaflet",
    "section": "",
    "text": "Living on the Colorado front range in summer means dealing with a chance of thunderstorms almost every afternoon, some of which can become severe.\nIn this post I’ll go over how I mapped the severe thunderstorm risk outlook from the NOAA Storm Prediction Center , using R and leaflet.\n\nCodesuppressPackageStartupMessages(library(sf))\nlibrary(leaflet)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(htmltools)"
  },
  {
    "objectID": "posts/Storm_Prediction_Center/Severe_Weather_Mapping.html#downloading-the-data",
    "href": "posts/Storm_Prediction_Center/Severe_Weather_Mapping.html#downloading-the-data",
    "title": "Mapping Severe Thunderstorm Outlook in R using Leaflet",
    "section": "Downloading the data",
    "text": "Downloading the data\nThe shapefiles can be downloaded manually by clicking the link on the website, but the link changes whenever the forecast is updated. I wanted to do this in a more programmatic way and be able to automatically find the link for the latest forecast without having to manually copy and paste the link each time I run it. I experimented with using the SelectorGadget to isolate the link, but found it easier to create a list of all the links in the page using the rvest (Wickham 2022) package and then find the link containing the shapefile (ends in .shp.zip).\n\nCodebase_url &lt;- \"https://www.spc.noaa.gov\"\n\n\n# Define a function to make a list of all links on a webpage\nget_all_links &lt;- function(page_url){\n  \n  # read the html from the website for day 1 outlook\n  html &lt;- rvest::read_html(page_url)\n  \n  # create a list of all the hyperlinks on the website\n  links &lt;- rvest::html_attr(html_nodes(html, \"a\"), \"href\")\n  \n}\n\nlinks &lt;- get_all_links(page_url=paste0(base_url,\"/products/outlook/day1otlk.html\"))\n\n# find the link for the shapefile (they only one that ends in 'shp.zip') \nshp_link &lt;- links[which(stringr::str_ends(links,'shp.zip'))]\nshp_url &lt;- paste0(base_url,shp_link)\nprint(paste('The latest shapefile as of ',Sys.time(),' is ',shp_url))\n\n[1] \"The latest shapefile as of  2023-07-06 10:20:49  is  https://www.spc.noaa.gov/products/outlook/archive/2023/day1otlk_20230706_1300-shp.zip\"\n\nCode# filename of shapefile\nshp_fname &lt;- basename(shp_url)\n#print(shp_fname)\n\n# base filename (remove *-shp.zip*) to use to load files later\nbasefname &lt;- stringr::str_remove(shp_fname,\"-shp.zip\")\n#print(basefname)\n\n\n\nNow that we have the link for the latest forecast, we can download the file (zip file) and unzip.\n\nThe unzipped folder contains shapefiles files for tornado,wind, and hail threat, but I will focus on just the categorical risk for severe thunderstorms (this is what you have probably seen on the weather forecast on the news). The shapfile I am interested in ends in cat.shp\nThen we can use the sf (Pebesma 2018) package to read the shapefile into R\n\n\nCode# destination to save downloaded file to\ndest_file &lt;- file.path('.','data',shp_fname)\n# download the zip file containing shapefiles\ndownload.file(url=shp_url,destfile = dest_file, method=\"curl\",quiet = TRUE)\n\n# unzip into a separate folder using base filename\nunzip(dest_file,exdir = file.path('.','data',basefname) )\n\n# read shapefile into R w/ sf package\ncat_file &lt;- stringr::str_remove(basename(shp_url),\"-shp.zip\")\ndat &lt;- sf::st_read(file.path('.','data',basefname,paste0(basefname,'_cat.shp')))\n\nReading layer `day1otlk_20230706_1300_cat' from data source \n  `/Users/andy/Projects/Andy_Pickering_Portfolio/posts/Storm_Prediction_Center/data/day1otlk_20230706_1300/day1otlk_20230706_1300_cat.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.85 ymin: 24.169 xmax: -67.471 ymax: 49.561\nGeodetic CRS:  WGS 84\n\n\nExamine the object extracted from the shapefile:\n\nCodeclass(dat)\n\n[1] \"sf\"         \"data.frame\"\n\nCodedat\n\nSimple feature collection with 4 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -123.85 ymin: 24.169 xmax: -67.471 ymax: 49.561\nGeodetic CRS:  WGS 84\n  DN        VALID       EXPIRE        ISSUE LABEL                     LABEL2\n1  2 202307061300 202307071200 202307061244  TSTM General Thunderstorms Risk\n2  3 202307061300 202307071200 202307061244  MRGL              Marginal Risk\n3  4 202307061300 202307071200 202307061244  SLGT                Slight Risk\n4  5 202307061300 202307071200 202307061244   ENH              Enhanced Risk\n   stroke    fill                       geometry\n1 #55BB55 #C1E9C1 POLYGON ((-92.46012 48.6844...\n2 #005500 #66A366 POLYGON ((-109.69 47.18, -1...\n3 #DDAA00 #FFE066 POLYGON ((-105.19 45.5, -10...\n4 #FF6600 #FFA366 POLYGON ((-102.51 39.91, -9..."
  },
  {
    "objectID": "posts/USPVDB/USPVDB_Analysis.html",
    "href": "posts/USPVDB/USPVDB_Analysis.html",
    "title": "Analysis of the U.S. Large-Scale Solar Photovoltaic Database (USPVDB) in R",
    "section": "",
    "text": "I recently learned of the U.S. Large-Scale Solar Photovoltaic Database (Fujita et al. 2023) via the Data Is Plural newsletter, and was excited to explore the data. The database includes information about U.S. ground-mounted photovoltaic (PV) facilities with capacity of 1 megawatt or more. Note that there is an online data viewer available as well; here I will explore the data using R and Quarto."
  },
  {
    "objectID": "posts/USPVDB/USPVDB_Analysis.html#total",
    "href": "posts/USPVDB/USPVDB_Analysis.html#total",
    "title": "Analysis of the U.S. Large-Scale Solar Photovoltaic Database (USPVDB) in R",
    "section": "Total",
    "text": "Total\nFor the total analysis, I will group the data by year and summarize some of the fields (Table 2), including the number of sites opened and the total capacity of all sites.\n\nSummarize data by yearpv_yearly &lt;- pv |&gt;\n  filter(p_year &gt;= 2002) |&gt; # only 1 site opened before 2002 (in 1986)\n  group_by(p_year) |&gt;\n  summarize(tot_cap_dc = sum(p_cap_dc, na.rm = TRUE),\n            tot_cap_ac = sum(p_cap_ac, na.rm = TRUE),\n            n_sites = n(),\n            ) |&gt;\n  mutate(cum_cap_dc = cumsum(tot_cap_dc),\n         cum_cap_ac = cumsum(tot_cap_ac),\n        cum_n_sites = cumsum(n_sites)\n         )\n\npv_yearly |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 2: Data aggregated/summarized by year\n\n\n\nNumber of sites opened and capacity added per year\nThe number of PV sites opened per year (Figure 2) began to increase rapidly around 2007 and has remained high (more than 400) since 2015. The total capacity added is more variable, reflecting the fact that some sites are much larger than others.\n\nCodep1 &lt;- pv_yearly |&gt;\n  ggplot(aes(p_year, n_sites)) +\n  geom_col() +\n  labs(title = \"Number of sites opened by year\",\n       x = \"Year\")\n\np2 &lt;- pv_yearly |&gt;\n  ggplot(aes(p_year, tot_cap_dc)) +\n  geom_col() +\n  labs(title = \"Total DC capacity opened by year\",\n       x = \"Year\",\n       y = \"DC Capacity [MW]\")\n\ncowplot::plot_grid(p1, p2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 2: (A) Number of sites opened per year (B) Total capacity (DC) opened per year\n\n\n\n\nCumulative sums\nNext we can look at the cumulative sum of the number of PV sites and total capacity (Figure 3). Both began to increase around 2007, and really took off starting around 2012.\n\nCodep1 &lt;- pv_yearly |&gt;\n  filter(p_year &gt; 2006) |&gt;\n  ggplot(aes(p_year, cum_n_sites)) +\n  geom_area(fill = 'gray') +\n  geom_line(linewidth = 2) +\n  labs(title = \"Number of PV Sites\",\n       x = 'Year',\n       y = \"# Sites\")\n\np2 &lt;- pv_yearly |&gt;\n  filter(p_year &gt; 2006) |&gt;\n  ggplot(aes(p_year, cum_cap_dc)) +\n  geom_area(fill = 'gray') +\n  geom_line(linewidth = 2) +\n  labs(title = \"PV Capacity\",\n       x = 'Year',\n       y = \"DC Capacity [MW]\")\n\ncowplot::plot_grid(p1, p2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 3: (A) Cumulative sum of total number of sites over time (B) Cumulative sum of total capacity (DC) over time"
  },
  {
    "objectID": "posts/USPVDB/USPVDB_Analysis.html#per-state-analysis",
    "href": "posts/USPVDB/USPVDB_Analysis.html#per-state-analysis",
    "title": "Analysis of the U.S. Large-Scale Solar Photovoltaic Database (USPVDB) in R",
    "section": "Per-State Analysis",
    "text": "Per-State Analysis\nIn this section I will analyze the data per state, so I will group by state and compute totals (Table 3). I’ve chosen not to also group by year in this sections, so the summary values are the total for all years included in the data (basically the current status).\n\nSummarize data by State# summarize pv data by state\npv_states &lt;- pv |&gt;\n  group_by(p_state) |&gt;\n  summarise(n_pv = n(),\n            tot_cap_dc = sum(p_cap_dc, na.rm = TRUE),\n            tot_cap_ac = sum(p_cap_ac, na.rm = TRUE),\n            avg_site_cap_dc = round(mean(p_cap_dc, na.rm = TRUE),2)\n            )\n\n\npv_states |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 3: Table of data grouped and summarized by state\n\n\n\nNumber of sites and total capacity per state\nFigure 4 shows the top ten states by number of PV sites and total capacity.\n\nIt is interesting to note that the states with the most sites do not always have the most capacity, reflecting that some states tend to have fewer but larger PV sites. For example, MA has 4th most sites, but is 12th in terms of total capacity.\n\n\nCodep1 &lt;- pv_states |&gt;\n  mutate(p_state = fct_reorder(p_state, n_pv)) |&gt;\n  slice_max(order_by = n_pv, n = 10) |&gt;\n  ggplot(aes(p_state, n_pv)) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Number of sites\",\n       x = \"State\")\n\np2 &lt;- pv_states |&gt;\n  mutate(p_state = fct_reorder(p_state, tot_cap_dc)) |&gt;\n  slice_max(order_by = tot_cap_dc, n = 10) |&gt;\n  ggplot(aes(p_state, tot_cap_dc)) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Total capacity of PV sites [MW]\",\n       x = \"State\")\n\ncowplot::plot_grid(p1, p2, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 4: (A) Number of PV sites per state (top 10 shown) (B) Total Capacity (DC) of PV sites per state (top 10 shown)"
  },
  {
    "objectID": "posts/USPVDB/USPVDB_Analysis.html#choropleths",
    "href": "posts/USPVDB/USPVDB_Analysis.html#choropleths",
    "title": "Analysis of the U.S. Large-Scale Solar Photovoltaic Database (USPVDB) in R",
    "section": "Choropleths",
    "text": "Choropleths\nNext I will make some choropleth maps to help visualize the state data. In these figures, the color of each state corresponds to a variable. For now I will restrict the maps to the lower 48 US states to make the plotting easier.\n\nThe maps are made using the leaflet (Cheng et al. 2023b) package and are interactive; you can drag the map and zoom in/out, and clicking on a state will display some information about it.\nI will get the state shapefiles for plotting from the tigris (Walker 2023) package.\n\nNumber of sites per state\n\nCodemap_val &lt;- \"n_pv\"\ndat_to_map &lt;- df_comb |&gt;\n  rename(val_to_map = all_of(map_val))\n\n# make color palette\ncol_pal &lt;- leaflet::colorNumeric(palette = \"viridis\",\n                                 domain = dat_to_map$val_to_map)\n\nleaflet() %&gt;% \n#  addTiles() %&gt;% # adds OpenStretMap basemap\n  addPolygons(data = dat_to_map,\n              weight = 1,\n              color = \"black\",\n              popup = paste(dat_to_map$STUSPS, \"&lt;br&gt;\",\n                            \" # Sites: \", dat_to_map$val_to_map, \"&lt;br&gt;\"),\n              fillColor = ~col_pal(val_to_map),\n              fillOpacity = 0.6) %&gt;% \n  addLegend(data = dat_to_map,\n            pal = col_pal,\n            values = ~val_to_map,\n            opacity = 1,\n            title = \"# PV Sites &lt;br&gt;\n            Per State\"\n            )\n\n\n\n\n\n\nFigure 5: Choropleth of the number of PV sites per state (lower 48 only)\n\n\n\nTotal Capacity per state\n\nCodemap_val &lt;- \"tot_cap_dc\"\ndat_to_map &lt;- df_comb |&gt;\n  rename(val_to_map = all_of(map_val))\n\n# make color palette\ncol_pal &lt;- leaflet::colorNumeric(palette = \"viridis\",\n                                 domain = dat_to_map$val_to_map)\n\nleaflet() %&gt;% \n#  addTiles() %&gt;% # adds OpenStretMap basemap\n  addPolygons(data = dat_to_map,\n              weight = 1,\n              color = \"black\",\n              popup = paste(dat_to_map$STUSPS, \"&lt;br&gt;\",\n                            \" PV Cap DC: \", dat_to_map$val_to_map,\"MW\", \"&lt;br&gt;\"),\n              fillColor = ~col_pal(val_to_map),\n              fillOpacity = 0.6) %&gt;% \n  addLegend(data = dat_to_map,\n            pal = col_pal,\n            values = ~val_to_map,\n            opacity = 1,\n            title = \"DC Capacity [MW] &lt;br&gt;\n            Per State\"\n            )\n\n\n\n\n\n\nFigure 6: Choropleth of total PV capacity (DC) per state (lower 48 only)\n\n\n\nAverage capacity per state\nSome states tend to have fewer but larger sites, and vice-versa. Figure 7 shows the average site capacity for each state.\n\nCodemap_val &lt;- \"avg_site_cap_dc\"\ndat_to_map &lt;- df_comb |&gt;\n  rename(val_to_map = all_of(map_val))\n\n# make color palette\ncol_pal &lt;- leaflet::colorNumeric(palette = \"viridis\",\n                                 domain = dat_to_map$val_to_map)\n\nleaflet() %&gt;% \n#  addTiles() %&gt;% # adds OpenStretMap basemap\n  addPolygons(data = dat_to_map,\n              weight = 1,\n              color = \"black\",\n              popup = paste(dat_to_map$STUSPS, \"&lt;br&gt;\",\n                            \" Avg Cap: \", dat_to_map$val_to_map,\"MW\" ,\"&lt;br&gt;\"),\n              fillColor = ~col_pal(val_to_map),\n              fillOpacity = 0.6) %&gt;% \n  addLegend(data = dat_to_map,\n            pal = col_pal,\n            values = ~val_to_map,\n            opacity = 1,\n            title = \"Average Site Cap [MW] &lt;br&gt;\n            Per State\"\n            )\n\n\n\n\n\n\nFigure 7: Choropleth of average site capacity (MW DC) per state (lower 48 only)"
  },
  {
    "objectID": "posts/EV_Stations/index.html",
    "href": "posts/EV_Stations/index.html",
    "title": "EV Charging Stations Analysis",
    "section": "",
    "text": "Recently I’ve been interested in analyzing trends in electric vehicle (EV) charging stations, using data from the Alternative Fuels Data Center’s Alternative Fuel Stations database. In this first post I’ll go over retrieving the data via an API, getting it into a tidy format, and some initial analysis and visualization.\n\nI’ll retrieve the EV station data using the AFDC API. The documentation for the AFDC fuel-stations API can be found at: https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/all/#station-count-record-fields\n\n\nYou can obtain a free API key at: [https://developer.nrel.gov/signup/]. I’ve saved my API key in my local .Renviron file so I can load it without exposing the key in my code.\n\n\n\n\n\n\nTip\n\n\n\nI find the easiest way to store an API key in your .Renviron file is with the usethis (Wickham, Bryan, et al. 2023) package. Simply call the usethis::edit_r_environ() function to open the .Renviron file, add your key in the form API_KEY=“xxxxx”, and save. After you restart R, you will be able to load the key in your code using Sys.getenv(\"API_KEY\").\n\n\n\nI will request data for all EV stations in Colorado.\nI’ll retrieve the data from the API using the httr (Wickham 2023) package.\n\n\nCode# API key is stored in my .Renviron file\napi_key &lt;- Sys.getenv(\"AFDC_KEY\")\n\n# base url for AFDC alternative fuel stations API\ntarget &lt;- \"https://developer.nrel.gov/api/alt-fuel-stations/v1\"\n\n# Return data for all electric stations in Colorado\napi_path &lt;- \".json?&fuel_type=ELEC&state=CO&limit=all\"\n\ncomplete_api_path &lt;- paste0(target,api_path,'&api_key=',api_key)\n\nresponse &lt;- httr::GET(url = complete_api_path)\n\nif (response$status_code != 200) {\n print(paste('Warning, API call returned error code',response$status_code))\n}\n\nprint(paste(\"Data last downloaded on\", response$date, \"with response code of\",response$status_code))\n\n[1] \"Data last downloaded on 2023-12-21 18:16:03 with response code of 200\"\n\n\n\nThe result returned from the API is a response object, and the data is in JSON format. The response (which I’m not printing here because would show my API key) contains a status code; a code of 200 means the API request was successful. Some of the general error codes the API might return are described here.\nI’ll use the jsonlite (Ooms 2014) package to convert the JSON to R.\n\n\nCodeev_dat &lt;- jsonlite::fromJSON(httr::content(response,\"text\"))\n\nclass(ev_dat)\n\n[1] \"list\"\n\nCodenames(ev_dat)\n\n[1] \"station_locator_url\" \"total_results\"       \"station_counts\"     \n[4] \"fuel_stations\"      \n\n\n\nThe converted response is actually a list containing the data as well as some metadata about the request.\nThe total_results field gives the total number of fuel station records that match your requested query (regardless of any limit applied).\n\n\nCodeev_dat$total_results\n\n[1] 2318\n\n\n\n\nThe *station_counts* field gives a breakdown by fuel type (here I requested only electric so the counts for all other fuel types are zero).\n\ntotal includes the number of individual chargers/plugs, which is why it is greater than the station count.\nIn this case, there are 2318 stations, and a total of 5737 chargers/plugs.\n\n\n\n\nCodeev_dat$station_counts$fuels$ELEC\n\n$total\n[1] 5737\n\n$stations\n$stations$total\n[1] 2318\n\n\nFinally, the data we want to analyze is in the fuel_stations data frame.\n\nCodeev &lt;- ev_dat$fuel_stations\n\n\n\nThe returned data contains many non-electric fields that we don’t need (they will all be NA since we requested electric fuel type only), so I’ll remove the non-relevant fields from the data frame to clean things up a bit, using the starts_with function from the dplyr (Wickham, François, et al. 2023) package.\nI’ll also change the date column type and add a variable for year opened, since I want to look at how many stations were opened over time.\n\nCode# filter out non-EV related fields\nev &lt;- ev %&gt;% select(-dplyr::starts_with(\"lng\")) %&gt;% \n  select(-starts_with(\"cng\")) %&gt;%\n  select(-starts_with(\"lpg\")) %&gt;%\n  select(-starts_with(\"hy\")) %&gt;% \n  select(-starts_with(\"ng\")) %&gt;% \n  select(-starts_with(\"e85\")) %&gt;% \n  select(-starts_with(\"bd\")) %&gt;% \n  select(-starts_with(\"rd\")) %&gt;% \n  filter(status_code == 'E') |&gt;\n  select(-ends_with(\"_fr\")) # drop french language columns\n\n\n# change date field to date type and add a year opened variable\nev$open_date &lt;- lubridate::ymd(ev$open_date)\nev$open_year &lt;- lubridate::year(ev$open_date)\n\n\n\n\n\nFirst I’d like to look at how many EV stations opened over time, so I’ll make a new data frame summarizing the number of stations opened by year.\n\nCodeev_opened &lt;- ev %&gt;% \n  count(open_year,name = \"nopened\")  %&gt;% \n  filter(!is.na(open_year)) \n\nev_opened |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 1: Number of charging stations opened per year.\n\n\n\n\nCodeev_opened %&gt;% ggplot(aes(open_year, nopened)) + \n  geom_col() +\n  xlab(\"Year Opened\") +\n  ylab(\"# Stations Opened\") +\n  ggtitle('EV Stations Opened in Colorado Each Year',subtitle = paste(\"Data downloaded on\", response$date)) +\n  theme_grey(base_size = 15) +\n  geom_text(aes(label = nopened), vjust = 0)\n\n\n\n\n\n\nFigure 1: Number of EV Charging Stations Opened In Colorado each year\n\n\n\n\n\nWe can also look at the cumulative sum of stations opened over time\n\nCodeev_opened %&gt;% ggplot(aes(open_year,cumsum(nopened))) +\n  geom_line(linewidth = 1.5) +\n  xlab(\"Year\") +\n  ylab(\"# Stations\") +\n  ggtitle(\"Cumulative sum of EV stations opened in CO\") +\n  theme_grey(base_size = 15)\n\n\n\n\n\n\nFigure 2: Cumulative sum of EV stations opened in CO\n\n\n\n\n\nNext I want to dig a little deeper and break down the station openings by charger type and/or level. I’d expect to see more Level 2 chargers in earlier years, and an increase in DC fast charging stations in more recent years. I’ll make a new data frame with the number of chargers opened by year, grouped by charging level (Level 1, Level 2, or DC fast).\n\n\nNote here I’m working with the number of chargers of each level, not the number of stations.\n\n\nCodeev_opened_level &lt;- ev %&gt;% \n  select(id,open_date,\n         open_year,\n         ev_dc_fast_num,\n         ev_level2_evse_num,ev_level1_evse_num) %&gt;%\n  group_by(open_year) %&gt;%\n  summarize(n_DC = sum(ev_dc_fast_num,na.rm = TRUE), \n            n_L2 = sum(ev_level2_evse_num,na.rm = TRUE),\n            n_L1 = sum(ev_level1_evse_num,na.rm = TRUE) ) %&gt;% \n  filter(!is.na(open_year))\n\nev_opened_level |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 2: Number of EV chargers opened per year, by charging level\n\n\n\nTo make plotting easier, I’ll pivot the dataframe from wide to long format so I can group by charging level:\n\nCodeev_opened_level_long &lt;- ev_opened_level %&gt;% \n  tidyr::pivot_longer(cols = c('n_DC','n_L2','n_L1'),\n                      names_to = \"Level\",\n                      names_prefix = \"n_\",\n                      values_to = \"n_opened\")\n\nev_opened_level_long |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 3: Number of EV chargers opened per year by charging level, in long format for plotting.\n\n\n\nNow I can go ahead and plot the number of chargers opened over time, by level.\n\nCodeg &lt;- ev_opened_level_long %&gt;% \n  ggplot(aes(open_year, n_opened, group = Level)) +\n  geom_line(aes(col = Level), linewidth = 1.5) +\n  geom_point(aes(col = Level), size = 4) +\n  xlab(\"Year Opened\") +\n  ylab(\"# Charges Opened\") +\n  ggtitle(\"Number of Chargers Opened Per Year By Level\")\n  \nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 3: Number of Chargers Opened Per Year By Level\n\n\n\n\n\nCodesessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Denver\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] dplyr_1.1.3    ggplot2_3.4.4  jsonlite_1.8.7 httr_1.4.7    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      compiler_4.3.1    renv_1.0.3        tidyselect_1.2.0 \n [5] tidyr_1.3.0       jquerylib_0.1.4   scales_1.2.1      yaml_2.3.7       \n [9] fastmap_1.1.1     R6_2.5.1          labeling_0.4.3    generics_0.1.3   \n[13] curl_5.1.0        knitr_1.44        htmlwidgets_1.6.2 tibble_3.2.1     \n[17] munsell_0.5.0     lubridate_1.9.3   bslib_0.5.1       pillar_1.9.0     \n[21] rlang_1.1.1       utf8_1.2.4        DT_0.30           cachem_1.0.8     \n[25] xfun_0.40         sass_0.4.7        lazyeval_0.2.2    viridisLite_0.4.2\n[29] plotly_4.10.3     timechange_0.2.0  cli_3.6.1         withr_2.5.1      \n[33] magrittr_2.0.3    crosstalk_1.2.0   digest_0.6.33     grid_4.3.1       \n[37] rstudioapi_0.15.0 lifecycle_1.0.3   vctrs_0.6.4       data.table_1.14.8\n[41] evaluate_0.22     glue_1.6.2        farver_2.1.1      fansi_1.0.5      \n[45] colorspace_2.1-0  purrr_1.0.2       rmarkdown_2.25    ellipsis_0.3.2   \n[49] tools_4.3.1       pkgconfig_2.0.3   htmltools_0.5.6.1"
  },
  {
    "objectID": "posts/EV_Stations/index.html#data",
    "href": "posts/EV_Stations/index.html#data",
    "title": "EV Charging Stations Analysis",
    "section": "",
    "text": "I’ll retrieve the EV station data using the AFDC API. The documentation for the AFDC fuel-stations API can be found at: https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/all/#station-count-record-fields\n\n\nYou can obtain a free API key at: [https://developer.nrel.gov/signup/]. I’ve saved my API key in my local .Renviron file so I can load it without exposing the key in my code.\n\n\n\n\n\n\nTip\n\n\n\nI find the easiest way to store an API key in your .Renviron file is with the usethis (Wickham, Bryan, et al. 2023) package. Simply call the usethis::edit_r_environ() function to open the .Renviron file, add your key in the form API_KEY=“xxxxx”, and save. After you restart R, you will be able to load the key in your code using Sys.getenv(\"API_KEY\").\n\n\n\nI will request data for all EV stations in Colorado.\nI’ll retrieve the data from the API using the httr (Wickham 2023) package.\n\n\nCode# API key is stored in my .Renviron file\napi_key &lt;- Sys.getenv(\"AFDC_KEY\")\n\n# base url for AFDC alternative fuel stations API\ntarget &lt;- \"https://developer.nrel.gov/api/alt-fuel-stations/v1\"\n\n# Return data for all electric stations in Colorado\napi_path &lt;- \".json?&fuel_type=ELEC&state=CO&limit=all\"\n\ncomplete_api_path &lt;- paste0(target,api_path,'&api_key=',api_key)\n\nresponse &lt;- httr::GET(url = complete_api_path)\n\nif (response$status_code != 200) {\n print(paste('Warning, API call returned error code',response$status_code))\n}\n\nprint(paste(\"Data last downloaded on\", response$date, \"with response code of\",response$status_code))\n\n[1] \"Data last downloaded on 2023-12-21 18:16:03 with response code of 200\"\n\n\n\nThe result returned from the API is a response object, and the data is in JSON format. The response (which I’m not printing here because would show my API key) contains a status code; a code of 200 means the API request was successful. Some of the general error codes the API might return are described here.\nI’ll use the jsonlite (Ooms 2014) package to convert the JSON to R.\n\n\nCodeev_dat &lt;- jsonlite::fromJSON(httr::content(response,\"text\"))\n\nclass(ev_dat)\n\n[1] \"list\"\n\nCodenames(ev_dat)\n\n[1] \"station_locator_url\" \"total_results\"       \"station_counts\"     \n[4] \"fuel_stations\"      \n\n\n\nThe converted response is actually a list containing the data as well as some metadata about the request.\nThe total_results field gives the total number of fuel station records that match your requested query (regardless of any limit applied).\n\n\nCodeev_dat$total_results\n\n[1] 2318\n\n\n\n\nThe *station_counts* field gives a breakdown by fuel type (here I requested only electric so the counts for all other fuel types are zero).\n\ntotal includes the number of individual chargers/plugs, which is why it is greater than the station count.\nIn this case, there are 2318 stations, and a total of 5737 chargers/plugs.\n\n\n\n\nCodeev_dat$station_counts$fuels$ELEC\n\n$total\n[1] 5737\n\n$stations\n$stations$total\n[1] 2318\n\n\nFinally, the data we want to analyze is in the fuel_stations data frame.\n\nCodeev &lt;- ev_dat$fuel_stations\n\n\n\nThe returned data contains many non-electric fields that we don’t need (they will all be NA since we requested electric fuel type only), so I’ll remove the non-relevant fields from the data frame to clean things up a bit, using the starts_with function from the dplyr (Wickham, François, et al. 2023) package.\nI’ll also change the date column type and add a variable for year opened, since I want to look at how many stations were opened over time.\n\nCode# filter out non-EV related fields\nev &lt;- ev %&gt;% select(-dplyr::starts_with(\"lng\")) %&gt;% \n  select(-starts_with(\"cng\")) %&gt;%\n  select(-starts_with(\"lpg\")) %&gt;%\n  select(-starts_with(\"hy\")) %&gt;% \n  select(-starts_with(\"ng\")) %&gt;% \n  select(-starts_with(\"e85\")) %&gt;% \n  select(-starts_with(\"bd\")) %&gt;% \n  select(-starts_with(\"rd\")) %&gt;% \n  filter(status_code == 'E') |&gt;\n  select(-ends_with(\"_fr\")) # drop french language columns\n\n\n# change date field to date type and add a year opened variable\nev$open_date &lt;- lubridate::ymd(ev$open_date)\nev$open_year &lt;- lubridate::year(ev$open_date)"
  },
  {
    "objectID": "posts/EV_Stations/index.html#analysis",
    "href": "posts/EV_Stations/index.html#analysis",
    "title": "EV Charging Stations Analysis",
    "section": "",
    "text": "First I’d like to look at how many EV stations opened over time, so I’ll make a new data frame summarizing the number of stations opened by year.\n\nCodeev_opened &lt;- ev %&gt;% \n  count(open_year,name = \"nopened\")  %&gt;% \n  filter(!is.na(open_year)) \n\nev_opened |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 1: Number of charging stations opened per year.\n\n\n\n\nCodeev_opened %&gt;% ggplot(aes(open_year, nopened)) + \n  geom_col() +\n  xlab(\"Year Opened\") +\n  ylab(\"# Stations Opened\") +\n  ggtitle('EV Stations Opened in Colorado Each Year',subtitle = paste(\"Data downloaded on\", response$date)) +\n  theme_grey(base_size = 15) +\n  geom_text(aes(label = nopened), vjust = 0)\n\n\n\n\n\n\nFigure 1: Number of EV Charging Stations Opened In Colorado each year\n\n\n\n\n\nWe can also look at the cumulative sum of stations opened over time\n\nCodeev_opened %&gt;% ggplot(aes(open_year,cumsum(nopened))) +\n  geom_line(linewidth = 1.5) +\n  xlab(\"Year\") +\n  ylab(\"# Stations\") +\n  ggtitle(\"Cumulative sum of EV stations opened in CO\") +\n  theme_grey(base_size = 15)\n\n\n\n\n\n\nFigure 2: Cumulative sum of EV stations opened in CO\n\n\n\n\n\nNext I want to dig a little deeper and break down the station openings by charger type and/or level. I’d expect to see more Level 2 chargers in earlier years, and an increase in DC fast charging stations in more recent years. I’ll make a new data frame with the number of chargers opened by year, grouped by charging level (Level 1, Level 2, or DC fast).\n\n\nNote here I’m working with the number of chargers of each level, not the number of stations.\n\n\nCodeev_opened_level &lt;- ev %&gt;% \n  select(id,open_date,\n         open_year,\n         ev_dc_fast_num,\n         ev_level2_evse_num,ev_level1_evse_num) %&gt;%\n  group_by(open_year) %&gt;%\n  summarize(n_DC = sum(ev_dc_fast_num,na.rm = TRUE), \n            n_L2 = sum(ev_level2_evse_num,na.rm = TRUE),\n            n_L1 = sum(ev_level1_evse_num,na.rm = TRUE) ) %&gt;% \n  filter(!is.na(open_year))\n\nev_opened_level |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 2: Number of EV chargers opened per year, by charging level\n\n\n\nTo make plotting easier, I’ll pivot the dataframe from wide to long format so I can group by charging level:\n\nCodeev_opened_level_long &lt;- ev_opened_level %&gt;% \n  tidyr::pivot_longer(cols = c('n_DC','n_L2','n_L1'),\n                      names_to = \"Level\",\n                      names_prefix = \"n_\",\n                      values_to = \"n_opened\")\n\nev_opened_level_long |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 3: Number of EV chargers opened per year by charging level, in long format for plotting.\n\n\n\nNow I can go ahead and plot the number of chargers opened over time, by level.\n\nCodeg &lt;- ev_opened_level_long %&gt;% \n  ggplot(aes(open_year, n_opened, group = Level)) +\n  geom_line(aes(col = Level), linewidth = 1.5) +\n  geom_point(aes(col = Level), size = 4) +\n  xlab(\"Year Opened\") +\n  ylab(\"# Charges Opened\") +\n  ggtitle(\"Number of Chargers Opened Per Year By Level\")\n  \nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 3: Number of Chargers Opened Per Year By Level"
  },
  {
    "objectID": "posts/EV_Stations/index.html#session-info",
    "href": "posts/EV_Stations/index.html#session-info",
    "title": "EV Charging Stations Analysis",
    "section": "",
    "text": "CodesessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Denver\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] dplyr_1.1.3    ggplot2_3.4.4  jsonlite_1.8.7 httr_1.4.7    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      compiler_4.3.1    renv_1.0.3        tidyselect_1.2.0 \n [5] tidyr_1.3.0       jquerylib_0.1.4   scales_1.2.1      yaml_2.3.7       \n [9] fastmap_1.1.1     R6_2.5.1          labeling_0.4.3    generics_0.1.3   \n[13] curl_5.1.0        knitr_1.44        htmlwidgets_1.6.2 tibble_3.2.1     \n[17] munsell_0.5.0     lubridate_1.9.3   bslib_0.5.1       pillar_1.9.0     \n[21] rlang_1.1.1       utf8_1.2.4        DT_0.30           cachem_1.0.8     \n[25] xfun_0.40         sass_0.4.7        lazyeval_0.2.2    viridisLite_0.4.2\n[29] plotly_4.10.3     timechange_0.2.0  cli_3.6.1         withr_2.5.1      \n[33] magrittr_2.0.3    crosstalk_1.2.0   digest_0.6.33     grid_4.3.1       \n[37] rstudioapi_0.15.0 lifecycle_1.0.3   vctrs_0.6.4       data.table_1.14.8\n[41] evaluate_0.22     glue_1.6.2        farver_2.1.1      fansi_1.0.5      \n[45] colorspace_2.1-0  purrr_1.0.2       rmarkdown_2.25    ellipsis_0.3.2   \n[49] tools_4.3.1       pkgconfig_2.0.3   htmltools_0.5.6.1"
  },
  {
    "objectID": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html",
    "href": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html",
    "title": "How has time-of-use pricing affected our home electricity use?",
    "section": "",
    "text": "Time of Use (TOU) electricity pricing can shift customer usage patterns and help manage supply-demand imbalances\nBut it may also introduce artificial spikes in demand that could strain the local grid.\nThe financial savings (~$2/month for my home) may not be sufficient to motivate most people to make changes requiring much effort."
  },
  {
    "objectID": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html#downloading-data",
    "href": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html#downloading-data",
    "title": "How has time-of-use pricing affected our home electricity use?",
    "section": "Downloading data",
    "text": "Downloading data\nUnfortunately Xcel does not have a public API or a way to request a date-range of data, so I had to download a hourly data file for each day individually. I downloaded hourly data for a ~ 1 month period and combined the files into a single data frame in a separate script.\nOur current Xcel TOU rate categories during the winter are :\n\nWeekdays 1-3pm: mid-peak ($0.15/kWh)\nWeekdays 3-7pm: peak ($0.19/kWh)\nAll other times (and holidays): off-peak ($0.12/kWh)\n\nTable 1 shows an example of the data I working with. I’ve added columns for the hour, day of week, and an indicator for whether it is a weekday or weekend. Ideally I would have data from before TOU pricing was implemented to compare to, but this is not possible because TOU pricing began soon after we got our smart meter. I will instead compare weekdays and weekends, since there is no peak pricing on weekends.\n\nLoad libraries and read datasuppressPackageStartupMessages(library(tidyverse, quietly = TRUE))\nggplot2::theme_set(theme_grey(base_size = 16))\nsuppressPackageStartupMessages(library(here))\nsuppressPackageStartupMessages(library(DT))\n\ndf &lt;- readRDS('data/hourly_elec_xcel_combined') |&gt;\n  select(-c(category)) |&gt;\n  mutate(rate_category = as.factor(rate_category)) |&gt;\n  mutate(rate_category = forcats::fct_relevel(rate_category, c(\"on_peak\",\"mid_peak\",\"off_peak\"))) \n\n\n\nCodedf |&gt;\n  head() |&gt;\n  DT::datatable(options = list(pageLength = 5), rownames = FALSE)\n\n\n\n\n\n\n\n\nTable 1: Hourly electricity usage data"
  },
  {
    "objectID": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html#summary",
    "href": "posts/Time_of_use_electricity/xcel_hourly_electric_analysis.html#summary",
    "title": "How has time-of-use pricing affected our home electricity use?",
    "section": "Summary",
    "text": "Summary\n\nUnder TOU pricing, we have shifted our electricity usage and tend to use less during peak pricing.\nOn many days, this shift has created a new spike in demand at the end of peak pricing (7pm weekdays), as well as a sharp decrease in demand at the beginning of the peak pricing period.\nThis spike in demand could have negative consequences for the local energy infrastructure, as discussed in a recent blog post from EnergyHub .\n\nQuestions/observations /discussion\n\nYou might be wondering what the actual $ savings of using TOU pricing are and if the financial incentives are worth it? Excel gives an option to opt-out of TOU pricing, in which case the flat rate is currently $0.13/kWh. I did a quick calculation comparing our total cost with TOU vs using the same amount of electricity at the flat rate, and the savings were about $2 for a month. Not negligible, but probably not enough to motivate someone to put a ton of effort into changing their patterns.\nPersonally, I am more motivated by the environmental impacts. Reducing usage during peak demand periods could avoid having to fire up peaker plants that often run on fossil fuels, although it is difficult to know if that is actually happening. Ideally I would be able to see a real-time feed of the electricity generation fuel mix (for example Platte River Power Authority in northern Colorado), and shift my usage to times when there is more clean/renewable generation. As far as I know, Xcel does not currently provide this information publicly.\nI would expect that this artificial spike issue will get worse as more households get EVs and install home chargers. Note that we are using Level 1 charging, this effect would be larger for those with home Level 2 chargers that charge at a faster rate."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html",
    "href": "posts/EV_station_mapping/ev_station_mapping.html",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "",
    "text": "As you may know from a previous post I am interested in electric-vehicle (EV) trends and the transition to a more electrified transportation fleet. I wanted to do some mapping and spatial analysis, and I recently took the Creating Maps with R course by Charlie Joey Hadley, so I decided to use some of the skills I learned to create some maps of EV charging station data for Colorado.\n\n\nMy goal in this post is to create choropleth map(s) showing the number of EV charging stations per county in Colorado."
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#goal",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#goal",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "",
    "text": "My goal in this post is to create choropleth map(s) showing the number of EV charging stations per county in Colorado."
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#ev-stations-data",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#ev-stations-data",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "EV Stations data",
    "text": "EV Stations data\nData on EV stations is obtained from the Alternative Fuels Data Center’s Alternative Fuel Stations database. See my previous post for more details on getting the data from the API.\n\nLoad EV stations data from API# API key is stored in my .Renviron file\napi_key &lt;- Sys.getenv(\"AFDC_KEY\")\n\ntarget &lt;- \"https://developer.nrel.gov/api/alt-fuel-stations/v1\"\n\n# Return data for all electric stations in Colorado\napi_path &lt;- \".json?&fuel_type=ELEC&state=CO&limit=all\"\n\ncomplete_api_path &lt;- paste0(target,api_path,'&api_key=',api_key)\n\nresponse &lt;- httr::GET(url = complete_api_path)\n\nif (response$status_code != 200) {\n print(paste('Warning, API call returned error code', response$status_code))\n}\n\n\nev_dat &lt;- jsonlite::fromJSON(httr::content(response,\"text\"))\n\nev &lt;- ev_dat$fuel_stations\n\n# filter out non-EV related fields\nev &lt;- ev %&gt;% select(-dplyr::starts_with(\"lng\")) %&gt;% \n  select(-starts_with(\"cng\")) %&gt;%\n  select(-starts_with(\"lpg\")) %&gt;%\n  select(-starts_with(\"hy\")) %&gt;% \n  select(-starts_with(\"ng\")) %&gt;% \n  select(-starts_with(\"e85\")) %&gt;% \n  select(-starts_with(\"bd\")) %&gt;% \n  select(-starts_with(\"rd\")) %&gt;% \n  filter(status_code == 'E')"
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#county-data",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#county-data",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "County data",
    "text": "County data\nNext I need shape files for the Colorado counties to make the map; these are obtained from the tigris (Walker 2023) package.\n\nCodeoptions(tigris_use_cache = TRUE)\nco_counties &lt;- tigris::counties(\"CO\",cb = TRUE, progress_bar = FALSE)\n\nRetrieving data for the year 2021\n\nCodehead(co_counties)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0603 ymin: 36.99902 xmax: -104.6606 ymax: 39.92525\nGeodetic CRS:  NAD83\n    STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID      NAME         NAMELSAD\n7        08      037 00198134 0500000US08037 08037     Eagle     Eagle County\n150      08      059 00198145 0500000US08059 08059 Jefferson Jefferson County\n151      08      067 00198148 0500000US08067 08067  La Plata  La Plata County\n165      08      077 00198154 0500000US08077 08077      Mesa      Mesa County\n174      08      035 00198133 0500000US08035 08035   Douglas   Douglas County\n209      08      043 00198137 0500000US08043 08043   Fremont   Fremont County\n    STUSPS STATE_NAME LSAD      ALAND   AWATER                       geometry\n7       CO   Colorado   06 4362754228 18970639 MULTIPOLYGON (((-107.1137 3...\n150     CO   Colorado   06 1979735379 25071495 MULTIPOLYGON (((-105.0558 3...\n151     CO   Colorado   06 4376255277 25642579 MULTIPOLYGON (((-108.3796 3...\n165     CO   Colorado   06 8621348071 31991710 MULTIPOLYGON (((-109.0603 3...\n174     CO   Colorado   06 2176020279  6795841 MULTIPOLYGON (((-105.3274 3...\n209     CO   Colorado   06 3972613670  2235542 MULTIPOLYGON (((-106.0123 3..."
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#zip-codes",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#zip-codes",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "Zip codes",
    "text": "Zip codes\nI have the EV station data and the county shape files, so the next step is to join them together. However, I have a problem: the EV stations data does not contain the county name or code, so I can’t join them yet without a common column. There are probably a lot of different solutions to this problem (for example the EV data contains addresses so I could geo-code these to get the county). In this case, I decided the easiest solution was to download the zip code database from the USPS (free for personal use), which contains both zip codes and their corresponding county (Table 1).\n\nCodezips &lt;- readr::read_csv(\"data/zip_code_database.csv\",\n                        show_col_types = FALSE) %&gt;% \n  filter(state == \"CO\") %&gt;% \n  select(zip, primary_city, county)\n\nzips |&gt;\n  DT::datatable(options = list(pageLength = 5), rownames = FALSE)\n\n\n\n\n\n\n\n\nTable 1: Zip code data from USPS\n\n\n\nNext I compute the number of stations per zip code in the EV data, and join to the zip code database to add the county column (Table 2).\n\nCodeev_county_counts &lt;- ev %&gt;% \n  select(id,zip,city) %&gt;% \n  left_join(zips, by = \"zip\") %&gt;% \n  dplyr::count(county) %&gt;% \n  arrange(desc(n))\n\nev_county_counts |&gt;\n  DT::datatable(options = list(pageLength = 5), rownames = FALSE)\n\n\n\n\n\n\n\n\nTable 2: Number of EV charging stations per county"
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#combining-data",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#combining-data",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "Combining data",
    "text": "Combining data\nNow we can finally join the data we want to plot (# EV stations per county) in ev_county_counts to our sf object (co_counties) with the county spatial data, and we are ready to make some maps.\n\nCodeco_ev_counts &lt;- co_counties %&gt;% \n  left_join(ev_county_counts, by = c(\"NAMELSAD\" = \"county\"))\n\nco_ev_counts &lt;- sf::st_transform(co_ev_counts, 4326)\n\nhead(co_ev_counts)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0603 ymin: 36.99902 xmax: -104.6606 ymax: 39.92525\nGeodetic CRS:  WGS 84\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID      NAME         NAMELSAD\n1      08      037 00198134 0500000US08037 08037     Eagle     Eagle County\n2      08      059 00198145 0500000US08059 08059 Jefferson Jefferson County\n3      08      067 00198148 0500000US08067 08067  La Plata  La Plata County\n4      08      077 00198154 0500000US08077 08077      Mesa      Mesa County\n5      08      035 00198133 0500000US08035 08035   Douglas   Douglas County\n6      08      043 00198137 0500000US08043 08043   Fremont   Fremont County\n  STUSPS STATE_NAME LSAD      ALAND   AWATER   n                       geometry\n1     CO   Colorado   06 4362754228 18970639 106 MULTIPOLYGON (((-107.1137 3...\n2     CO   Colorado   06 1979735379 25071495 172 MULTIPOLYGON (((-105.0558 3...\n3     CO   Colorado   06 4376255277 25642579  39 MULTIPOLYGON (((-108.3796 3...\n4     CO   Colorado   06 8621348071 31991710  53 MULTIPOLYGON (((-109.0603 3...\n5     CO   Colorado   06 2176020279  6795841  62 MULTIPOLYGON (((-105.3274 3...\n6     CO   Colorado   06 3972613670  2235542   6 MULTIPOLYGON (((-106.0123 3..."
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#ggplot",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#ggplot",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "ggplot",
    "text": "ggplot\n\nggplot2 makes it relatively easy to plot spatial data in an sf object with the geom_sf function\nThe scales (Wickham and Seidel 2022) package is used to format the numbers in the legend\nThe ggspatial (Dunnington 2023) package is used to add a base map showing some of the major cities and roads\n\n\nCodeggplot() +\n  ggspatial::annotation_map_tile(progress = \"none\") +\n  geom_sf(data = co_ev_counts,\n          aes(fill = n),\n          alpha = 0.5) +\n  scale_fill_viridis_c(labels = scales::number_format(big.mark = \",\"),\n                       name = '# Ev Stations') +\n  ggtitle(\"Number of EV Stations by Colorado County\") +\n  theme_void()\n\n\n\n\n\n\nFigure 1: Choropleth map of number of EV charging stations by county, made with ggplot2"
  },
  {
    "objectID": "posts/EV_station_mapping/ev_station_mapping.html#leaflet",
    "href": "posts/EV_station_mapping/ev_station_mapping.html#leaflet",
    "title": "Mapping the Number of EV Charging Stations by County in Colorado Using R",
    "section": "Leaflet",
    "text": "Leaflet\nUsing leaflet requires a little more code but allows you to create an interactive map that can be more useful to the reader.\n\nIn the map below (Figure 2) I’ve set the popup to display the county name and number of stations when you click on the map.\nYou can also drag the map around and zoom in/out.\nIt’s also very easy with Leaflet to add a basemap (OpenStreetMap in this case) layer under the choropleth. I decided to add this here to give readers a better sense of context, and also because I wanted to highlight that the counties close to major highways (I-70 east-west and I-25 north-south) appear to have higher numbers of chargers.\nNote I’ve also included some code using from the Creating Maps in R course to fix an issue in the legend where the NA entry overlaps with the other entries.\n\n\nCode# create color palette\npal_ev &lt;- leaflet::colorNumeric(palette = \"viridis\",\n                                 domain = co_ev_counts$n)\n\nco_ev_map &lt;- leaflet() %&gt;% \n  addTiles() %&gt;% # adds OpenStretMap basemap\n  addPolygons(data = co_ev_counts,\n              weight = 1,\n              color = \"black\",\n              popup = paste(co_ev_counts$NAME, \"&lt;br&gt;\",\n                            \" EV Stations: \", co_ev_counts$n, \"&lt;br&gt;\"),\n              fillColor = ~pal_ev(n),\n              fillOpacity = 0.6) %&gt;% \n  addLegend(data = co_ev_counts,\n            pal = pal_ev,\n            values = ~n,\n            opacity = 1,\n            title = \"# of EV Stations &lt;br&gt;\n            Per County\"\n            )\n\n# legend fix --------------------------------------------------------------\n# for issue with na in legend\nhtml_fix &lt;- htmltools::tags$style(type = \"text/css\", \"div.info.legend.leaflet-control br {clear: both;}\") \n\nco_ev_map %&gt;% \n  htmlwidgets::prependContent(html_fix)\n\n\n\n\n\n\n\nFigure 2: Interactive choropleth map of number of EV charging stations by county"
  },
  {
    "objectID": "posts/EIA_degree_days/degreedays.html",
    "href": "posts/EIA_degree_days/degreedays.html",
    "title": "Analyzing Trends in Heating and Cooling Degree days using R",
    "section": "",
    "text": "Degree days are useful as a measure of building heating and cooling demands. A degree day is calculated as the difference between the average temperature (the average of the high and low temperature for the day) and a reference temperature (in the US 65°F is used). For example, if the average temperature today is 40°F, that would be 25 heating degree days (HDD). A summer day with an average temperature of 85°F would have 20 cooling degree days (CDD). Degree days are usually well correlated with the amount of energy used to heat or cool a home.\nI was interested in obtaining and analyzing degree day data; in particular I wanted to see if there were any noticeable trends over time. Given an overall increase in earth’s average temperature due to climate change, I would hypothesize that there might be an overall increase in CDD and a decrease in HDD.\nChanges in heating or cooling degree days would have implications for the amount of energy needed in the future to heat and cool residential or commercial buildings, resulting changes in demand on the electric grid, and implications for related carbon emissions (either for the power grid or from burning fossil fuels to heat buildings)."
  },
  {
    "objectID": "posts/EIA_degree_days/degreedays.html#heating-degree-days",
    "href": "posts/EIA_degree_days/degreedays.html#heating-degree-days",
    "title": "Analyzing Trends in Heating and Cooling Degree days using R",
    "section": "Heating Degree Days",
    "text": "Heating Degree Days\nFigure 1 shows the distribution (using a boxplot) of US heating degree days for each month. Not surprisingly HDD tends to be higher in winter months, although there is a decent amount of variability between years.\nHDD Per Month\n\nCodedd |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(month_name, HDD, group = month_name)) +\n  geom_boxplot() +\n  labs(title = 'Monthly Heating Degree Days for US (1997-2022)',\n       x = 'Month',\n       y = \"Heating Degree Days\")\n\n\n\n\n\n\nFigure 1: Boxplot of US heating degree days for each month\n\n\n\n\nTrends in HDD\nIs there a trend in HDD over time? I would expect that HDD might decrease over time due to climate change and the increase in earth’s average temperature.\nAnnual\nFigure 2 shows a timeseries of the annual total heating degree days in the US, along with a linear regression line that shows a negative trend.\n\nCodeg &lt;- dd_yearly |&gt;\n  ggplot(aes(year, HDD)) +\n  geom_point(size = 4, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y~x') +\n  labs(title = \"US Annual Heating Degree Days\")\n\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 2: Timeseries of annual US HDD\n\n\n\n\nCodehdd_yearly_fit &lt;- lm(data = dd_yearly, formula = 'HDD ~ year')\n\nbroom::tidy(hdd_yearly_fit) |&gt; mutate_if(is.numeric,~round(.x,digits = 3)) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nTable 3: Results of linear regression for annual HDD\n\n\n\nThere is a fair bit of variability, but looking at the fit metrics (Table 3) shows that the negative trend in HDD is statistically significant (p-value &lt; 0.05). Annual heating degree days are decreasing at a rate of 13 HDD per year.\nMonthly\nWe have seen that there is a negative trend in annual HDD; what are the trends for individual months? Figure 3 shows timeseries of monthly HDD vs year for winter months, with linear regression lines plotted over them. Visually there appears to be a negative trend for some of the months.\n\nCodedd |&gt;\n  filter(month %in% c(11,12,1,2,3,4)) |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(year, HDD, group = month_name)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap('month_name', scales = 'free') +\n  guides(x =  guide_axis(angle = 45))\n\n\n\n\n\n\nFigure 3: HDD vs year for winter months\n\n\n\n\nTo better quantify these trends I want to fit a linear regression to the data for each month and examine the results. This could be done with a for loop, but I will take advantage of a nice nested workflow using the tidyr (Wickham, Vaughan, and Girlich 2023), broom (Robinson, Hayes, and Couch 2023), and purrr (Wickham and Henry 2023) packages.\n\nCodedd_fit_hdd &lt;- dd |&gt;\n  group_by(month) |&gt;\n  nest() |&gt;\n  mutate(fit = map(data, ~ lm(HDD ~ year, data = .x) ),\n         tidied = map(fit, broom::tidy),\n         glanced = map(fit, broom::glance)\n  ) %&gt;%\n  unnest(tidied) |&gt;\n  ungroup()\n\ndd_fit_hdd |&gt;\n  mutate_if(is.numeric,~round(.x,digits = 3)) |&gt;\n  DT::datatable(rownames = FALSE, options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 4: Results of linear regression fits of heating degree days for each month\n\n\n\nNow that the data and model fit results are in a tidy dataframe (Table 4), they can be easily filtered to identify significant fits using p-values (Table 5). The months with significant trends are June, August, September, and October. These months and the linear regression lines are shown in Figure 4\n\nCodedd_fit_hdd  |&gt;\n  filter(term == 'year') |&gt;\n  filter(p.value &lt; 0.05) |&gt;\n  mutate_if(is.numeric,~round(.x,digits = 3)) |&gt;\n  select(-c(data, fit, term, glanced)) |&gt;\n  DT::datatable(rownames = FALSE)\n\n\n\n\n\n\n\n\nTable 5: Table of significant HDD fits (based on p-value &lt; 0.05)\n\n\n\n\nCodedd |&gt;\n  filter(month %in% c(6,8,9,10)) |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(year, HDD, group = month_name)) +\n  geom_point(size = 4, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap('month_name', scales = 'free') \n\n\n\n\n\n\nFigure 4: HDD vs year for months with signficant trends"
  },
  {
    "objectID": "posts/EIA_degree_days/degreedays.html#cooling-degree-days",
    "href": "posts/EIA_degree_days/degreedays.html#cooling-degree-days",
    "title": "Analyzing Trends in Heating and Cooling Degree days using R",
    "section": "Cooling Degree Days",
    "text": "Cooling Degree Days\nCDD Per Month\nFigure 5 shows the distribution of US heating degree days for each month. Not surprisingly CDD tends to be higher in summer months, although there is a decent amount of variability between years.\n\nCodedd |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(month_name, CDD, group = month_name)) +\n  geom_boxplot() +\n  labs(title = 'Monthly Cooling Degree Days for US',\n       x = 'Month',\n       y = \"Cooling Degree Days\")\n\n\n\n\n\n\nFigure 5: Boxplot of US cooling degree days for each month\n\n\n\n\nTrends in CDD\nAnnual\nIs there a trend in CDD over time? I would expect that CDD might increase over time due to climate change and the increase in earth’s average temperature.\nFigure 6 shows a timeseries of the annual total heating degree days in the US, along with a linear regression line showing a positive trend.\n\nCodeg &lt;- dd_yearly |&gt;\n  ggplot(aes(year, CDD)) +\n  geom_point(size = 4, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y~x') +\n  labs(title = \"US Annual Cooling Degree Days\")\n\nplotly::ggplotly(g)\n\n\n\n\n\n\nFigure 6: Timeseries of annual US CDD\n\n\n\n\nCodecdd_yearly_fit &lt;- lm(data = dd_yearly, formula = 'CDD ~ year')\n\nbroom::tidy(cdd_yearly_fit) |&gt; \n  mutate_if(is.numeric,~round(.x,digits = 3)) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\nTable 6: Results of linear regression for annual CDD\n\n\n\nLooking at the fit metrics shows that the positive trend in CDD is indeed statistically significant (Table 6), with CDD increasing at a rate of 12.28\nMonthly\nFigure 7 shows timeseries of monthly CDD vs year for the 4 summer months with the highest CDD, with linear regression lines plotted over them. Visually there appears to be a positive trend for each month.\n\nCodedd |&gt;\n  filter(month %in% c(6,7,8,9)) |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(year, CDD, group = month_name)) +\n  geom_point(size = 4, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap('month_name')\n\n\n\n\n\n\nFigure 7: CDD vs year for summer month\n\n\n\n\nNext I’ll apply a linear regression to each month using the same workflow used for HDD.\n\nCodedd_fit_cdd &lt;- dd |&gt;\n  group_by(month) |&gt;\n  nest() |&gt;\n  mutate(fit = map(data, ~ lm(CDD ~ year, data = .x) ),\n         tidied = map(fit, broom::tidy)\n  ) %&gt;%\n  unnest(tidied) |&gt;\n  ungroup()\n\n\nThere are significant positive trends in CDD for July, August, September, October, and December (Table 7). Figure 8 shows the data and fits for these months in detail.\n\nCodedd_fit_cdd  |&gt;\n  filter(term == 'year') |&gt;\n  filter(p.value &lt; 0.05) |&gt;\n  mutate_if(is.numeric,~round(.x,digits = 3)) |&gt;\n  select(-c(data, fit, term)) |&gt;\n  DT::datatable(rownames = FALSE,options = list(pageLength = 5))\n\n\n\n\n\n\n\n\nTable 7: Table of significant CDD fits\n\n\n\n\nCodedd |&gt;\n  filter(month %in% c(7,8,9,10,12)) |&gt;\n  mutate(month_name = lubridate::month(date, label = TRUE)) |&gt;\n  ggplot(aes(year, CDD, group = month_name)) +\n  geom_point(size = 4, alpha = 0.5) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap('month_name', scales = 'free') +\n  guides(x =  guide_axis(angle = 45))\n\n\n\n\n\n\nFigure 8: CDD vs year for Months with significant trends"
  },
  {
    "objectID": "posts/EIA_degree_days/degreedays.html#implications-for-energy-use",
    "href": "posts/EIA_degree_days/degreedays.html#implications-for-energy-use",
    "title": "Analyzing Trends in Heating and Cooling Degree days using R",
    "section": "Implications for energy use",
    "text": "Implications for energy use\n\nHow much actual energy use (kWh, therms, etc.) corresponds to a degree day? This will depend on many factors, but we could make a rough estimate by comparing degree days to energy demand or consumption."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my (new) data science portfolio and blog, made with Quarto and RStudio.\nIf you’re interested in setting up your own Quarto website or blog, I highly recommend watching this tutorial by Deepsha Menghani.\nYou can still view my previous blog; I am in the process of figuring out how/if I will move that content here."
  },
  {
    "objectID": "posts/EIA_Electric_Fuel_Breakdown/index.html",
    "href": "posts/EIA_Electric_Fuel_Breakdown/index.html",
    "title": "Calculating State Electricity Generation By Fuel Type using R",
    "section": "",
    "text": "Electricity generation is a major source of carbon emissions, and transitioning to cleaner and/or renewable sources of power generation is important to lowering greenhouse gas emissions and limiting the effects of climate change (as well as other benefits such as improving air quality). With the increase in electrification (such as electric vehicles and heat pumps), transitioning to lower-carbon sources of electricity generation is even more important.\nIn general electric vehicles produce less net emissions than traditional gas-powered vehicles, but the savings depends on several factors including how the electricity used to charge an EV is produced. The Alternative Fuels Data Center has a nice tool showing the breakdown of electricity generation fuel sources by state and the resulting effects on emissions.\nI am interested in analyzing trends in electricity generation fuel sources, and this post outlines the first step: Using data from the U.S. Energy Information Administration (EIA) to calculate and reproduce the state-level breakdown in the AFDC tool.\nSome of the technical methods/topics involved in this include:\n\nRetrieving data from an API using the httr (Wickham 2023b) package\nPivoting data between long and wide formats using the tidyr (Wickham, Vaughan, and Girlich 2023) package\nPlotting data using the ggplot2 (Wickham 2016) and plotly (Sievert 2020) packages\n\n\nCodelibrary(httr)\nlibrary(jsonlite)\nlibrary(ggplot2)\ntheme_set(theme_grey(base_size = 15)) # make the default font sizes etc a little bigger\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(forcats)\nsuppressPackageStartupMessages(library(plotly))\nlibrary(DT)\n\n\nI’m going to make a simple little function to retrieve data from the API, that will make things a little tidier and warn me if the API call returns an error code.\n\nCoderetrieve_api_data &lt;- function(api_path){\n  \n  response_raw &lt;- httr::GET(url=complete_api_path)\n  \n  if (response_raw$status_code!=200){\n    print(paste(\"Warning, API returned error code \",response_raw$status_code))\n  }\n  \n  return(response_raw)\n  \n}\n\n\nThe data I will use is the annual electric power generation by state from the EIA API. I’m going to just look at data for Colorado for now, and I’m looking at sector id 98: electric power.\n\nCode# API key stored in .Renviron\napi_key &lt;- Sys.getenv(\"EIA_KEY\")\n\n# base url for EIA API V2\napi_base &lt;- \"https://api.eia.gov/v2/\"\n\nroute &lt;- 'electricity'\nsubroute &lt;- \"electric-power-operational-data\"\ndata_name &lt;- 'generation'\n\nstate &lt;- 'CO'\n\n# sectorid 98= electric power \nsector_id &lt;- 98\n\n# annual\ncomplete_api_path &lt;- paste0(api_base,route,'/',subroute,'/','data/',\n                            '?frequency=annual&data[0]=',data_name,\n                            '&facets[sectorid][]=',sector_id,\n                            '&facets[location][]=',state,\n                            '&api_key=',api_key)\n\n# get the data from the API\nresponse_raw &lt;- retrieve_api_data(complete_api_path)\n\n# convert from JSON\ndat &lt;- jsonlite::fromJSON(httr::content(response_raw,\"text\"))\n\nNo encoding supplied: defaulting to UTF-8.\n\nCode# extract the dataframe \ndf &lt;- dat$response$data\n\n# rename a column and drop some extra unecessary columns\ndf &lt;- df %&gt;% rename(year=period) %&gt;% \n  select(-c(location,sectorid,sectorDescription,stateDescription))\n\nhead(df)\n\n  year fueltypeid fuelTypeDescription generation       generation-units\n1 2001        ALL           all fuels  46582.114 thousand megawatthours\n2 2001        BIT     bituminous coal         NA thousand megawatthours\n3 2001        DFO distillate fuel oil    158.640 thousand megawatthours\n4 2001        FOS        fossil fuels  45257.257 thousand megawatthours\n5 2001         NG         natural gas   9146.986 thousand megawatthours\n6 2001        ORW    other renewables     32.102 thousand megawatthours\n\n\nNote that some of the fueltype categories are subsets of, or overlap with, other categories. For example COW is all coal products, which includes SUB (subbituminous coal) and BIT (bituminous coal). For this analysis I will look at the following categories:\n\nALL\nCOW (all coal)\nNatural Gas\nWND : Wind\nSUN : Solar\nHYC: conventional hydroelectric\nBIO: Biomass\n\n\n\n\n\n\n\nNote\n\n\n\nNote that depending on the state not all fuel type fields are returned. For example Colorado currently has no nuclear production, but data for other states may include this.\n\n\n\nCodedf &lt;- df %&gt;% \n  filter(fueltypeid %in% c('ALL','FOS','REN','COW','SUN','WND','NG','HYC','BIO','HPS'))"
  },
  {
    "objectID": "posts/EIA_Electric_Fuel_Breakdown/index.html#pie-chart-using-ggplot2",
    "href": "posts/EIA_Electric_Fuel_Breakdown/index.html#pie-chart-using-ggplot2",
    "title": "Calculating State Electricity Generation By Fuel Type using R",
    "section": "Pie chart using ggplot2",
    "text": "Pie chart using ggplot2\n\nCodedf_perc_long %&gt;% \n  filter(year==\"2021\") %&gt;% \n  ggplot(aes(x=\"\",y=percent,fill=FuelType))+\n  geom_bar(stat=\"identity\", width=1) +\n  coord_polar(\"y\", start=0) +\n  theme_void()+\n  geom_text(aes(label = paste0(round(percent,2), \"%\")),\n            position = position_stack(vjust=0.5)) +\n  labs(x = NULL, y = NULL, fill = NULL)\n\n\n\nPie chart of the perecent of total electricity generation by fuel type"
  },
  {
    "objectID": "posts/EIA_Electric_Fuel_Breakdown/index.html#pie-chart-using-plotly",
    "href": "posts/EIA_Electric_Fuel_Breakdown/index.html#pie-chart-using-plotly",
    "title": "Calculating State Electricity Generation By Fuel Type using R",
    "section": "Pie chart using Plotly",
    "text": "Pie chart using Plotly\nI found that I was able to easily make a little bit nicer-looking and interactive chart with plotly :\n\nCodedata &lt;- df_perc_long %&gt;% \n  filter(year==\"2021\") \n\nfig &lt;- plot_ly(data, labels = ~FuelType, values = ~percent, type = 'pie')\nfig &lt;- fig %&gt;% layout(title = 'Electricity Generation By Fuel Type for Colorado 2021',\n         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),\n         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))\n\nfig\n\n\nPie chart of the perecent of total electricity generation by fuel type"
  },
  {
    "objectID": "posts/EIA_Electric_Fuel_Breakdown/index.html#would-a-bar-chart-be-better",
    "href": "posts/EIA_Electric_Fuel_Breakdown/index.html#would-a-bar-chart-be-better",
    "title": "Calculating State Electricity Generation By Fuel Type using R",
    "section": "Would a bar chart be better?",
    "text": "Would a bar chart be better?\nIn general, I’m not a huge fan of piecharts; it can be difficult to judge the relative values (areas) of the different segments. So I thought I’d try displaying the data as a bar chart too.\n\n\n\n\n\n\nTip\n\n\n\nIf you want the bars to plot in order by their values, you can use the fct_reorder function from the forcats library (Wickham 2023a) to re-order the factor levels.\n\n\n\nCodedf_perc_long %&gt;% \n  filter(year==\"2021\") %&gt;% \n  mutate(FuelType=forcats::fct_reorder(FuelType,percent)) %&gt;% \n  ggplot(aes(FuelType,percent))+\n  geom_col(aes(fill=FuelType))+\n  xlab(\"Fuel Type\")+\n  ylab(\"Percent of Total Electric Generation\")+\n  ggtitle(\"% of Electric Generation by Fuel Type (CO 2021)\")+\n  coord_flip()\n\n\n\nBar chart of the perecent of total electricity generation by fuel type"
  },
  {
    "objectID": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html",
    "href": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html",
    "title": "Trends in US Electricity Generation and CO2 Emissions",
    "section": "",
    "text": "In this analysis I will look at trends in US electricity generation and associated CO2 emissions, using a nice data set available from Ember. Ember provides monthly and yearly data on US electricity generation and emissions for the total US as well as by state, both in total and broken down by fuel types. In this post, I will limit my analysis to yearly data and total (all fuel types) US generation and emissions."
  },
  {
    "objectID": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#generation-data",
    "href": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#generation-data",
    "title": "Trends in US Electricity Generation and CO2 Emissions",
    "section": "Generation Data",
    "text": "Generation Data\n\nCodedf_gen_yearly_UStot |&gt;\n  gt::gt() |&gt; \n  opt_row_striping() |&gt; \n  opt_interactive(use_highlight = TRUE, page_size_default = 5)\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Annual electricity generation data for USA"
  },
  {
    "objectID": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#emissions-data",
    "href": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#emissions-data",
    "title": "Trends in US Electricity Generation and CO2 Emissions",
    "section": "Emissions Data",
    "text": "Emissions Data\n\nCodedf_emis_yearly_UStot |&gt; \n  gt::gt() |&gt; \n  opt_row_striping() |&gt; \n  opt_interactive(use_highlight = TRUE, page_size_default = 5)\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Annual US power-sector emissions."
  },
  {
    "objectID": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#further-exploration",
    "href": "posts/Ember_US_Electric_Total/Ember_US_elec_total.html#further-exploration",
    "title": "Trends in US Electricity Generation and CO2 Emissions",
    "section": "Further Exploration:",
    "text": "Further Exploration:\nSome of the areas I plan to explore in continuing analysis of this data include:\n\nBreaking down data by fuel types\nLooking at monthly data\nExamine correlation with weather/temperature\nLooking for changes in seasonal patterns of energy generation associated with home electrification (e.g. electric heat pumps replacing gas furnaces).\nBreak down by individual states"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting Trailhead Parking Fullness at JeffCo Open Space Trailheads : Using a model to predict parking lot usage at Jefferson County Open Space trailheads in Colorado.\nJournalists Under Fire : An analysis of global threats to journalists and press freedom."
  },
  {
    "objectID": "projects.html#a-list-of-some-of-my-data-science-projects",
    "href": "projects.html#a-list-of-some-of-my-data-science-projects",
    "title": "Projects",
    "section": "",
    "text": "Predicting Trailhead Parking Fullness at JeffCo Open Space Trailheads : Using a model to predict parking lot usage at Jefferson County Open Space trailheads in Colorado.\nJournalists Under Fire : An analysis of global threats to journalists and press freedom."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Pickering et al. (2015)\nPickering and Alford (2012)\nAlford et al. (2015)"
  },
  {
    "objectID": "publications.html#selected-scientific-publications",
    "href": "publications.html#selected-scientific-publications",
    "title": "Publications",
    "section": "",
    "text": "Pickering et al. (2015)\nPickering and Alford (2012)\nAlford et al. (2015)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Data Science Resources",
    "section": "",
    "text": "R-bloggers"
  },
  {
    "objectID": "resources.html#r",
    "href": "resources.html#r",
    "title": "Data Science Resources",
    "section": "",
    "text": "R-bloggers"
  },
  {
    "objectID": "resources.html#python",
    "href": "resources.html#python",
    "title": "Data Science Resources",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "resources.html#quarto",
    "href": "resources.html#quarto",
    "title": "Data Science Resources",
    "section": "Quarto",
    "text": "Quarto\n\nhttps://quarto.org/\nTutorial on Creating a Data Science Portfolio with Quarto\nMarkdown basics"
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Data Science Resources",
    "section": "Data",
    "text": "Data\n\nEnergy\n\nAlternative Fuels Data Center\nU.S. Energy Information Administration (EIA)\nOur World in Data\nEmber\n\n\n\nOther\n\nData Is Plural\nTidy Tuesday"
  },
  {
    "objectID": "resources.html#miscellaneous",
    "href": "resources.html#miscellaneous",
    "title": "Data Science Resources",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nA guide to standard HTTP Status Codes that may be returned when requesting data from an API."
  }
]